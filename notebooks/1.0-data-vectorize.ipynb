{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7be08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prepared conversations DataFrame\n",
    "\n",
    "DATA_INTERIM_PATH = os.path.join(os.getcwd(), '../data/interim')\n",
    "DATA_PROCESSED_PATH = os.path.join(os.getcwd(), '../data/processed')\n",
    "\n",
    "conv_df = pd.read_csv(os.path.join(os.getcwd(), DATA_INTERIM_PATH, 'preprocessed_conversations.csv'))\n",
    "\n",
    "conv_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4795223",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv_df.isna().sum())  # Check for any remaining NaN values\n",
    "conv_df.head() # Display the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the text in the 'text1' and 'text2' column of the DataFrame\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatize the input text.\"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in nltk.pos_tag(tokens):\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'n', 'v'] else None\n",
    "        if wntag:\n",
    "            lemmatized_tokens.append(lemma.lemmatize(word, pos=wntag))\n",
    "        else:\n",
    "            lemmatized_tokens.append(word)\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "conv_df[\"text1\"] = conv_df[\"text1\"].apply(lemmatize_text)\n",
    "conv_df[\"text2\"] = conv_df[\"text2\"].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a Vocabulary of the words in the conversations\n",
    "\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.trimmed = False\n",
    "        self.reset_vocab()\n",
    "\n",
    "    def reset_vocab(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def trim(self, min_count=1):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "        for word, count in self.word2count.items():\n",
    "            if count >= min_count:\n",
    "                keep_words.append(word)\n",
    "\n",
    "        self.reset_vocab()\n",
    "        for word in keep_words:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_words\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.word2index.get(item, None)\n",
    "        elif isinstance(item, int):\n",
    "            return self.index2word.get(item, None)\n",
    "        else:\n",
    "            raise TypeError(\"Item must be either a string or an integer.\")\n",
    "        \n",
    "    def __contains__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return item in self.word2index\n",
    "        elif isinstance(item, int):\n",
    "            return item in self.index2word\n",
    "        else:\n",
    "            raise TypeError(\"Item must be either a string or an integer.\")\n",
    "        \n",
    "vocab = Vocabulary()\n",
    "print(\"Vocabulary initialized with {} words.\".format(len(vocab)))\n",
    "print(\"Vocabulary contains the following special tokens:\", vocab[0], vocab[1], vocab[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81da1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the text from the DataFrame to the vocabulary\n",
    "\n",
    "def add_text_to_vocab(text):\n",
    "    \"\"\"\n",
    "    Adds the text to the vocabulary.\n",
    "    \"\"\"\n",
    "    vocab.add_sentence(text)\n",
    "\n",
    "conv_df[\"text1\"].apply(add_text_to_vocab)\n",
    "conv_df[\"text2\"].apply(add_text_to_vocab)\n",
    "\n",
    "print(\"Vocabulary size after adding text:\", len(vocab))\n",
    "\n",
    "idx = random.randint(0, len(vocab) - 1)\n",
    "print(\"Random word in vocabulary:\", vocab[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8212a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the vocabulary to only include words that appear at least MIN_COUNT times\n",
    "\n",
    "MIN_COUNT = 3\n",
    "def trim_vocab(vocab, min_count=MIN_COUNT):\n",
    "    \"\"\"\n",
    "    Trims the vocabulary to only include words that appear at least MIN_COUNT times.\n",
    "    \"\"\"\n",
    "    vocab.trim(min_count)\n",
    "    print(\"Vocabulary trimmed to {} words.\".format(len(vocab)))\n",
    "\n",
    "    # Now update the DataFrame to remove words not in the trimmed vocabulary\n",
    "    def filter_text(text):\n",
    "        return ' '.join([word for word in text.split() if word in vocab])\n",
    "    \n",
    "    conv_df_trimmed = conv_df.copy()\n",
    "    conv_df_trimmed[\"text1\"] = conv_df_trimmed[\"text1\"].apply(filter_text)\n",
    "    conv_df_trimmed[\"text2\"] = conv_df_trimmed[\"text2\"].apply(filter_text)\n",
    "\n",
    "    conv_df_trimmed = conv_df_trimmed[(conv_df_trimmed[\"text1\"].str.strip() != \"\") & (conv_df_trimmed[\"text2\"].str.strip() != \"\")]\n",
    "\n",
    "    return conv_df_trimmed\n",
    "\n",
    "# Trim the vocabulary and update the DataFrame\n",
    "conv_df_trimmed = trim_vocab(vocab, MIN_COUNT)\n",
    "\n",
    "# Vocabulary size after trimming: 23570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text in the DataFrame to vectors of indices based on the vocabulary\n",
    "\n",
    "def vectorize_text(text, vocab):\n",
    "    \"\"\"\n",
    "    Converts a text to a vector of indices based on the vocabulary.\n",
    "    \"\"\"\n",
    "    return [vocab[word] for word in text.split() if word in vocab]\n",
    "\n",
    "conv_df_trimmed[\"text1_vectorized\"] = conv_df_trimmed[\"text1\"].apply(lambda x: vectorize_text(x, vocab))\n",
    "conv_df_trimmed[\"text2_vectorized\"] = conv_df_trimmed[\"text2\"].apply(lambda x: vectorize_text(x, vocab))\n",
    "\n",
    "conv_df_vectorized = conv_df_trimmed[[\"text1_vectorized\", \"text2_vectorized\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a fixed length (for training purposes)\n",
    "\n",
    "def pad_sequence(sequence, max_length, pad_value=PAD_token):\n",
    "    \"\"\"\n",
    "    Pads a sequence to the specified max_length with the pad_value.\n",
    "    \"\"\"\n",
    "    return sequence + [pad_value] * (max_length - len(sequence))\n",
    "\n",
    "max_length = max(conv_df_vectorized[\"text1_vectorized\"].apply(len).max(), conv_df_vectorized[\"text2_vectorized\"].apply(len).max())\n",
    "\n",
    "print(\"Maximum sequence length for padding:\", max_length)\n",
    "\n",
    "conv_df_vectorized[\"text1_vectorized\"] = conv_df_vectorized[\"text1_vectorized\"].apply(lambda x: pad_sequence(x, max_length))\n",
    "conv_df_vectorized[\"text2_vectorized\"] = conv_df_vectorized[\"text2_vectorized\"].apply(lambda x: pad_sequence(x, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb17e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns for clarity\n",
    "conv_df_vectorized.rename(columns={\"text1_vectorized\": \"seq1\", \"text2_vectorized\": \"seq2\"}, inplace=True)\n",
    "\n",
    "# Before saving, we convert the list columns to string format for better compatibility with CSV\n",
    "conv_df_vectorized[\"seq1\"] = conv_df_vectorized[\"seq1\"].apply(lambda x: ','.join(map(str, x)))\n",
    "conv_df_vectorized[\"seq2\"] = conv_df_vectorized[\"seq2\"].apply(lambda x: ','.join(map(str, x)))\n",
    "\n",
    "conv_df_vectorized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd225cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed DataFrame and vocabulary to files\n",
    "\n",
    "conv_df_vectorized.to_csv(os.path.join(os.getcwd(), DATA_PROCESSED_PATH, 'conversations_vectorized.csv'), index=False)\n",
    "\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bda364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
